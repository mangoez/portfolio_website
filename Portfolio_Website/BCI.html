<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Po's Portfolio Website</title>
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/normalize/7.0.0/normalize.min.css">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.css" integrity="sha256-46qynGAkLSFpVbEBog43gvNhfrOj+BmwXdxFgVK/Kvc=" crossorigin="anonymous" />
        <link rel='shortcut icon' href='img/favicon.ico' type='image/x-icon' />

        <!-- Update these with your own fonts -->
        <link href="https://fonts.googleapis.com/css?family=Source+Code+Pro:400,900|Source+Sans+Pro:300,900&display=swap" rel="stylesheet">

        <link rel="stylesheet" href="css/style.css">

    </head>
    <body>
        <header>
            <div class="logo">
                <a href="https://www.youtube.com/watch?v=dQw4w9WgXcQ" target="_blank">
                    <img src="img/logo.png" alt="">
                </a>
            </div>
            <button class="nav-toggle" aria-label="toggle navigation">
                <span class="hamburger"></span>
            </button>
            <nav class="nav">
                <ul class="nav__list">
                    <li class="nav__item"><a href="index.html#home" class="nav__link">Home</a></li>
                    <li class="nav__item"><a href="index.html#about" class="nav__link">About me</a></li>
                    <li class="nav__item"><a href="index.html#services" class="nav__link">What I can do</a></li>
                    <li class="nav__item"><a href="index.html#work" class="nav__link">Projects, Qualifications & Experiences</a></li>
                    <li class="nav__item"><a href="index.html#progress" class="nav__link">Current progress</a></li>
                </ul>
            </nav>
        </header>


        <section class="intro">
            <h2 class="section__title section__title--intro">
                Real-time Motor Imagery Classifier <strong>Project</strong>
            </h2>
            <p class="section__subtitle section__subtitle--intro">Supervised by Dr Sam John</p>
            <img src="img/BCI/10_20.jpeg" alt="" class="intro__img">
        </section>

        <div class="portfolio-item-individual">
            <p>This is a project offered to me by Dr Sam John as I was looking for a way to break into the Brain
               Computer Interface field. He simply told me to read up, try out a Support Vector Machine on the
               raw data and here I am!</p>

            <p>This is using dataset IVc from the <a href="http://www.bbci.de/competition/iii/desc_IVc.html"> BCI competition </a> </p>
            <p>This is also my first data science project, you will see some some machine learning and learn
               a little neuroscience~</p>
            <p>The end goal is to have two people playing pong against each other only using motor imagery.</p>
        </div>

        <!-- Introduction -->
        <section class="about-me" id="about">
           <h2 class="section__title section__title--about">But how?</h2>
           <p class="section__subtitle section__subtitle--about">A little explanation</p>

           <div class="about-me__body">
               <p>Our target features are quantifying Event Related Desynchonisation (ERD) and Event Related Sychronisation (ERS)
                  in the EEG data. ERD is a decrease in signal power and ERS is an increase in signal power, from this we can distinguish when an individual is resting and
                  thinking about moving their left hand or right foot. What's more is this feature
                  is stronger during motor imagery than during movement.</p>

               <p>This feature has a few interesting properties where based on left hand/right foot motor imagery,
                  different areas of the will report a stronger ERD/ERS during the motor imagery period.
                  For example, We know that ERD is a contralateral event, so thinking about your left hand means
                  ERD will occur on the right side of the motor cortex more than the left side. <br><br>
                  To be concrete: </p>

               <p>On electrode C4 after the start of left hand motor imagery period you expect it to register a stronger decrease in signal
                  power - ERD, than electrode C3 because electrode C4 is
                  on the <strong>right side</strong> of the brain and hence responsible for the left side of your body.
               <p>If you were doing right hand motor imagery it would be the other way round!</p>

               <p>To illustrate, here is a plot of the Power Spectral Density for the trials of left hand/right foot.</p>
               <img src="img/BCI/PSD.png" alt="PSD" class="about-me__img">

               <p>We can see that for left hand motor imagery, the frequencies 7-12Hz are decreased on electrode C4 and not C3.
               This is the Event Related Desychronisation we are looking for regarding the hands. I am not sure what it is meant to be for feet.</p>
           </div>

           <img src="img/BCI/transparent.png" class="transparent">
        </section>


        <section class="about-me" id="about">
           <h2 class="section__title section__title--about">Data processing</h2>
           <p class="section__subtitle section__subtitle--about">Rereferencing, Filtering, CSP, PSD</p>

           <div class="about-me__body">
               <p>From the literature, traditionally C3 & C4 contain the best data so I will be using these two as my minimum. </p>
               <p>EEG also is notorious for a low signal to noise ratio, as it is notoriously susceptible to noise.
                  Below details the ways we can extract some features from the EEG and get the data to be spick and span~</p>


               <p> <strong>Filtering:</strong> <br>
                  The ERD features are reported to be in the bandwidth of around 7-12Hz with some patterns
                  emerging around the high 20s where the literature often does 7-30Hz for filtering. For this I chose a zero-phase butterworth filter so lets have a look
                  at it's frquency response on electrode C3 to make sure it doesn't cut our signal.</p>

               <img src="img/BCI/butterworth.png" alt="freq_resp" class="about-me__img">

               <p>So it looks like order 23 is starting to break, so let's do order 22 to maintain as much information.
                  We can confirm the filter worked by plotting the power spectral density (PSD)
                  with Welches periodogram.</p>

               <img src="img/BCI/PSD_check.png" class="transparent">
               <p>Clearly the temporal domain isn't very distinguishable (the time domain) by eye but it may be different
                  to an algorithm.<br><br>
                  So to illustrate the differences:</p>

               <img src="img/BCI/filt_compare.png" alt="filt_compare" class="about-me__img">


               <p><strong>Rereferencing</strong> <br>
                  This turned out to be the most important step as it made a huge difference to my data.
                  The idea behind re-referencing is to express the voltage of the EEG channels with respect to another,
                  this helps amplify inter-electrode patterns. </p>

               <p>There are many ways to do this, such as Common Average Referencing (CAR), Cz referencing, mastoid referencing,
                  the list goes on... won't bore you down with all of it, but I can show you what referencing
                  to Cz did to my signals as that is the one I chose.</p>

               <img src="img/BCI/ref_compare.png" alt="filt_compare" class="about-me__img">
               <p>The other (more popular method) CAR is when you subtract the average of all the
                  electrodes from the individual electrodes, this works because what you are essentially doing is an attempt at removing <thead>
                  noise that is common to all the electrodes</p>

               <p>However, in practice, to achieve such an ideal reference one would require large number of
                   electrodes that cover the whole head uniformly, which is not the case in EEG recordings where
                   limited number of electrodes cover mostly the upper part of the head</p>

               <p>To use as little electrodes as possible, Cz rereferencing will be used.</p>


               <p><strong>Power Spectral Density (PSD)</strong><br>
                  Now at this point we could just plug that into the classifier and train it, however it is still noisy so
                  we need a way to downsample it without losing too much information and isolate the features even more. </p>
               <p>So the simplest way is represet each trial in the frequency domain as we had demonstrated before:</p>

              <img src="img/BCI/psd.png" class="transparent">

              <p><strong>Epoching</strong></p>
              <p>Extract two, 1 second windows related to motor imagery and one
                 1 second window for rest from each trial.</p>
              <p>The first 1 second motor imagery window is 1 second after onset of cue.</p>
              <p>The second will be the 1 second window 2 seconds after onset of the cue which will hopefully
                 capture the subject in the later part of motor imagery.</p>
              <p>The 1 second of rest window will be taken 0.75 seconds after the end of the motor imagery section (3.5 seconds from cue)
                 to help give space for the subject to cool down from doing motor imagery.</p>

              <br><br>

              <p><strong>Final datasets</strong><br>
              <p>The dataset condition: </p>
              <ul>
                 <li>Number of electrodes: (C3, C4)</li>
                 <li>1 second windows of motor imagery and rest</li>
                 <li>Unfiltered PSD feature extraction</li>
                 <li>Filtered between 7-30Hz</li>
                 <li>210 trials of left hand, right foot and rest each making 630 total trials</li>
              </ul>





           </div>

           <img src="img/BCI/transparent.png" class="transparent">
        </section>


        <section class="about-me" id="about">
           <h2 class="section__title section__title--about">Classifying!</h2>
           <p class="section__subtitle section__subtitle--about">FINALLY, the cool part~</p>

           <div class="about-me__body">
               <p>Firstly, NO deep learning. Not nearly enough samples. So we will stick to a range of linear classifiers
                  which are used in the BCI literature: </p>
               <ul>
                   <li>SVM rbf kernel</li>
                   <li>Random Forest Classifier</li>
                   <li>Linear Discriminant Analysis: Shrinkage, Eigen Solver</li>
               </ul>
               <br><br>

               <p><strong>Splitting the data</strong></p>
               <p>We will split the data with two thirds to train the model and one third to test but keeping the trials in order from when they
                  were first and last recorded. This will mimic how it
                  would work in real life because you can't train on future trials and then classify in the past when doing this experiment in real time.</p>


               <p><strong>Results</strong></p>
               <p>It works!</p>
               <p>Kinda good enough ish...</p>

               <img src="img/BCI/RF.png" alt="filt_compare" class="about-me__img">
               <img src="img/BCI/LDA.png" alt="filt_compare" class="about-me__img">
               <img src="img/BCI/SVM.png" alt="filt_compare" class="about-me__img">

           </div>

           <img src="img/BCI/transparent.png" class="transparent">
        </section>


        <section class="about-me" id="about">
           <h2 class="section__title section__title--about">Final Product</h2>
           <p class="section__subtitle section__subtitle--about">It works!</p>

           <div class="about-me__body">
               <p>To show asychronous (continuous) decoding, instead of just getting the class as the output for each trial, we can use a sliding window of 1 sec
                  and pump out continuous classification of a trial from -1 second before onset of motor imagery to 2 seconds after the motor imagery has ended.</p>

               <p>This would be akin to initiating motor imagery at will as opposed to when a stimulus is presented!</p>
               <p>To recap, the motor imagery trials are 3.5s long and interspersed with 1.75s-2.25s of rest.
                  The images below are marked with the green line as the start of motor imagery and the red line as the end.</p>

               <p>So the classifier will output a value of 1 for right foot, -1 for left hand and 0 for rest. We can then use a smoothing algorithm to give us a
                  motor imagery scale/confidence of sorts.</p>

               <p>Some of the trials are shown below:</p>
               <img src="img/BCI/Trial149.png" alt="asynchronous output" class="about-me__img">
               <img src="img/BCI/Trial189.png" alt="asynchronous output" class="about-me__img">
           </div>

           <img src="img/BCI/transparent.png" class="transparent">
        </section>



        <!-- Footer -->
        <footer class="footer">
            <!-- replace with your own email address -->
            <a href="mailto:liupochen@gmail.com" class="footer__link">liupochen@gmail.com</a>
            <ul class="social-list">
                <li class="social-list__item">
                    <a class="social-list__link" href="https://www.linkedin.com/in/po-liu-science-slave/" target="black">
                        <i class="fab fa-linkedin"></i>
                    </a>
                </li>
            </ul>
        </footer>


        <script src="js/index.js"></script>

    </body>
</html>
